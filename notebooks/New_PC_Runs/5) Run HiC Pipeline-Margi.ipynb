{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMIT4DN fdnDCIC TOOLS ARE DEPRECIATED.\n",
      "      PLEASE SWITCH TO USING DCICUTILS INSTEAD.\n",
      "      fdnDCIC WILL BE DELETED JUNE 2018.\n"
     ]
    }
   ],
   "source": [
    "from core.utils import Tibanna\n",
    "from dcicutils import ff_utils\n",
    "from core.utils import run_workflow\n",
    "from datetime import datetime\n",
    "from core.wfr import *\n",
    "\n",
    "env = 'data'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key={\"default\" : tibanna.ff_keys})\n",
    "exclude_miseq = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 total number of sets\n",
      "0 sets completed\n",
      "\n",
      "({u'4DNEXWXUC1X2': [(u'/files-fastq/4DNFIHQ8L5W5/', u'/files-fastq/4DNFIEWU4VLJ/')], u'4DNEX21WVKWL': [(u'/files-fastq/4DNFI6CUSJVL/', u'/files-fastq/4DNFIA6RMVMW/')]}, u'human', None, '4DNFIZQZ39L9', '4DNFI823LSII', None, 5)\n",
      "1 4DNESJC437VS\n",
      "None human\n",
      "about to start run bwa-mem_4DNFIHQ8L5W5_4DNFIEWU4VLJ\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 5, 24, 17, 3, 58, 831000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'fa4176e0-5f95-11e8-91d8-67c2e9bbd86f', 'HTTPHeaders': {'x-amzn-requestid': 'fa4176e0-5f95-11e8-91d8-67c2e9bbd86f', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFIHQ8L5W5_4DNFIEWU4VLJ'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFIHQ8L5W5_4DNFIEWU4VLJ\n",
      "4DNEXWXUC1X2 has missing Part1 runs\n",
      "about to start run bwa-mem_4DNFI6CUSJVL_4DNFIA6RMVMW\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 5, 24, 17, 4, 30, 247000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '0cf97caf-5f96-11e8-be27-c963b14287ae', 'HTTPHeaders': {'x-amzn-requestid': '0cf97caf-5f96-11e8-be27-c963b14287ae', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFI6CUSJVL_4DNFIA6RMVMW'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFI6CUSJVL_4DNFIA6RMVMW\n",
      "4DNEX21WVKWL has missing Part1 runs\n",
      "Part3 not ready\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# for a given experiment set and some parameters like instrument\n",
    "# print set of files and their partA hic workflow status\n",
    "# if there are one that are running report the number of running cases\n",
    "# if there are file pairs that don't have a corresponding part A, report them separately\n",
    "\n",
    "out_n = \"This is an output file of the Hi-C processing pipeline\"\n",
    "int_n = \"This is an intermediate file in the HiC processing pipeline\"\n",
    "\n",
    "def step_settings(seq, my_organism):\n",
    "    genome = \"\"\n",
    "    mapper = {'human':'GRCh38','mouse':'GRCm38'}\n",
    "    genome = mapper.get(my_organism)\n",
    "    \n",
    "    wf_dict =[{\n",
    "        'wf_name': 'bwa-mem',\n",
    "        'wf_uuid': '3feedadc-50f9-4bb4-919b-09a8b731d0cc',\n",
    "        'parameters': {\"nThreads\": 16},\n",
    "        'custom_pf_fields': {\n",
    "            'out_bam': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'intermediate file',\n",
    "                'description': int_n}\n",
    "        }},\n",
    "        {\n",
    "        'wf_name': 'hi-c-processing-bam',\n",
    "        'wf_uuid': '023bfb3e-9a8b-42b9-a9d4-216079526f68',\n",
    "        'parameters': {\"nthreads_merge\": 16, \"nthreads_parse_sort\": 16},\n",
    "        'custom_pf_fields': {\n",
    "            'annotated_bam': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'alignment',\n",
    "                'description': out_n},\n",
    "            'filtered_pairs': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact list-replicate',\n",
    "                'description': out_n}\n",
    "        }},\n",
    "        {\n",
    "        'wf_name': 'hi-c-processing-pairs',\n",
    "        'wf_uuid': 'c9e0e6f7-b0ed-4a42-9466-cadc2dd84df0',\n",
    "        'parameters': {\"nthreads\": 1, \"maxmem\": \"32g\"},\n",
    "        'custom_pf_fields': {\n",
    "            'cooler_normvector': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'juicebox norm vector',\n",
    "                'description': out_n},\n",
    "            'hic': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact matrix',\n",
    "                'description': out_n},\n",
    "            'mcool': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact matrix',\n",
    "                'description': out_n},\n",
    "            'merged_pairs': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact list-combined',\n",
    "                'description': out_n}\n",
    "        }}]\n",
    "    \n",
    "    return wf_dict[seq]\n",
    "  \n",
    "    \n",
    "# url for hic exps\n",
    "exp_types = ['MARGI']\n",
    "set_url = '/search/?'+'&'.join(['experiments_in_set.experiment_type='+i for i in exp_types])+'&type=ExperimentSetReplicate'\n",
    "run_sets = ff_utils.get_metadata(set_url , connection=ff)['@graph']\n",
    "\n",
    "add_pc = False\n",
    "add_rel = False\n",
    "add_wfr = True\n",
    "\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "all_sets = len(run_sets)\n",
    "print(str(all_sets)+' total number of sets')\n",
    "\n",
    "run_sets = [i for i in run_sets if \"HiC_Pipeline_0.2.5\"  not in i.get('completed_processes', [])]\n",
    "\n",
    "print(str(all_sets-len(run_sets))+ ' sets completed')\n",
    "\n",
    "\n",
    "for a_set in run_sets: \n",
    "    counter += 1\n",
    "\n",
    "    print\n",
    "\n",
    "    fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size = find_pairs(a_set, exclude_miseq, ff, tibanna)\n",
    "    print(fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size)\n",
    "    if not bwa_ref or not chrsize_ref:\n",
    "        print counter, a_set['accession'], organism, enzyme, 'skipping set with not chrsize/bwa index'\n",
    "        continue\n",
    "    \n",
    "#     if not enz_ref:\n",
    "#         print counter, a_set['accession'], 'skipping not ready NZ', organism, enzyme\n",
    "#         continue\n",
    "    \n",
    "#     if f_size < 15:\n",
    "#         print counter, a_set['accession'], 'skipping small file size', str(f_size) \n",
    "#         continue\n",
    "        \n",
    "        \n",
    "    print counter, a_set['accession']\n",
    "    print enzyme, organism\n",
    "    part3 = 'done'\n",
    "    list_release = []\n",
    "    set_pairs = []        \n",
    "    # cycle through the experiments\n",
    "    for exp in fastqpairs.keys():\n",
    "        if not fastqpairs.get(exp):\n",
    "            print(exp, 'does not have any fastq pairs')\n",
    "            continue\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        part2 = 'done'\n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            bam1 = get_wfr_out(pair[0], 'bwa-mem 0.2.5', 'bam', ff)\n",
    "            bam2 = get_wfr_out(pair[1], 'bwa-mem 0.2.5', 'bam', ff)\n",
    "            # if run is not successful\n",
    "            if bam1.startswith('no') or not bam1 or bam1 != bam2:\n",
    "                part1 = 'not ready'\n",
    "                if add_wfr:\n",
    "                    if not bwa_index:\n",
    "                        print 'not yet usable', organism\n",
    "                        continue\n",
    "                    inp_f = {'fastq1':pair[0], 'fastq2':pair[1], 'bwa_index':bwa_ref}\n",
    "                    name_tag = pair[0].split('/')[2]+'_'+pair[1].split('/')[2]\n",
    "                    run_missing_wfr(step_settings(0, organism), inp_f, name_tag, ff, env, tibanna)\n",
    "            elif bam1 == 'running':\n",
    "                part1 = 'still running'\n",
    "                print('part1 still running')\n",
    "            # if successful\n",
    "            else:\n",
    "                exp_bams.append(bam1)\n",
    "                list_release.append(bam1)\n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            print exp, 'has missing Part1 runs'\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print exp, 'part1 complete'\n",
    "        #check if part 2 is run already, it not start the run\n",
    "        exp_com_bam = []\n",
    "        exp_pairs = []\n",
    "        for bam in exp_bams:\n",
    "            com_bam = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', 'bam', ff)\n",
    "            pairs = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', 'pairs', ff)\n",
    "            # try to run if missing\n",
    "            if pairs.startswith('no') or not pairs:\n",
    "                part2 = 'not ready'\n",
    "                part3 = 'not ready'\n",
    "                \n",
    "            elif pairs == 'running':\n",
    "                print(bam)\n",
    "                part2 = 'still running'\n",
    "                part3 = 'not ready'\n",
    "                \n",
    "            else:\n",
    "                exp_com_bam.append(com_bam)\n",
    "                exp_pairs.append(pairs)\n",
    "                \n",
    "        # if still running, skip to next experiment\n",
    "        if part2 == 'still running':\n",
    "            print('part2 still running')\n",
    "            continue\n",
    "        \n",
    "        # make sure all bams went through the same wfr and produces same file\n",
    "        if part2 != 'done' or len(list(set(exp_com_bam))) != 1 or len(list(set(exp_pairs))) !=1:\n",
    "            print exp, 'Part2 did not complete'\n",
    "            part3 = 'not ready' \n",
    "        \n",
    "            if add_wfr:\n",
    "                if not chrsize_ref:\n",
    "                    print 'not yet usable', organism\n",
    "                    continue\n",
    "                # make sure no duplicates\n",
    "                inp_f = {'input_bams':exp_bams, 'chromsize':chrsize_ref}           \n",
    "                run_missing_wfr(wf_dict[1], inp_f, exp, ff, env, tibanna)   \n",
    "            continue\n",
    "            \n",
    "        # add bam and pairs to exp proc file\n",
    "        list_release.extend([exp_com_bam[0],exp_pairs[0]])\n",
    "        if add_pc:\n",
    "            add_processed_files(exp, [exp_com_bam[0],exp_pairs[0]], ff)\n",
    "        \n",
    "        print exp, 'part2 complete'\n",
    "        set_pairs.append(exp_pairs[0])\n",
    "    \n",
    "    if part3 != 'done':\n",
    "        print 'Part3 not ready'\n",
    "        continue\n",
    "    \n",
    "    if not set_pairs:\n",
    "        print 'no pairs can be produced from this set'\n",
    "        continue\n",
    "        \n",
    "    merged_pairs = []\n",
    "    for set_pair in set_pairs:\n",
    "        merged_pair = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'pairs', ff)\n",
    "        hic = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'hic', ff)\n",
    "        mcool = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'mcool', ff)\n",
    "        normvec = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'normvector_juicerformat', ff)\n",
    "        if merged_pair.startswith('no') or not merged_pair:\n",
    "            part3 = 'not ready'\n",
    "            break\n",
    "        elif merged_pair == 'running':\n",
    "            part3 = 'still running'\n",
    "            break\n",
    "        else:\n",
    "            merged_pairs.append(merged_pair)\n",
    "    \n",
    "    \n",
    "    # if part3 is still running report it, and skip the rest of the script\n",
    "    if part3 == 'still running':\n",
    "        print 'part3', part3\n",
    "        continue        \n",
    "                \n",
    "    if part3 != 'done' or len(list(set(merged_pairs))) != 1:\n",
    "        print a_set['accession'], 'is missing Part3'\n",
    "        \n",
    "        # if it is not run, and add_wfr is true, go for it, then skip the rest of the script\n",
    "        if add_wfr:\n",
    "            if not chrsize_ref:\n",
    "                print 'not yet usable', organism\n",
    "                continue\n",
    "\n",
    "            if not enz_ref:\n",
    "                print 'restriction enzyme not ready for', organism, enzyme\n",
    "                continue\n",
    "            inp_f = {'input_pairs':set_pairs, 'chromsizes':chrsize_ref, 'restriction_file': enz_ref} \n",
    "            run_missing_wfr(wf_dict[2], inp_f, a_set['accession'], ff, env, tibanna)\n",
    "        continue\n",
    "    #####\n",
    "    #add competed flag to experiment\n",
    "    if add_pc and add_rel:\n",
    "        ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.5\"]}, obj_id=a_set['accession'] ,connection=ff)\n",
    "    \n",
    "    # add processed files to set\n",
    "    list_release.extend([merged_pair, hic, mcool, normvec])\n",
    "    if add_pc:\n",
    "        add_processed_files(a_set['accession'], [merged_pair, hic, mcool, normvec], ff)\n",
    "    \n",
    "    #release files and wfrs\n",
    "    if add_rel:\n",
    "        release_files(a_set['accession'], list(set(list_release)), ff)\n",
    "    \n",
    "    completed += 1\n",
    "    completed_acc.append(a_set['accession'])\n",
    "    print a_set['accession'], 'part3 complete'\n",
    "\n",
    "    \n",
    "print completed\n",
    "print completed_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
