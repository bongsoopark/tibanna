{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from dcicutils import ff_utils\n",
    "from core.utils import run_workflow\n",
    "from datetime import datetime\n",
    "from core.wfr import *\n",
    "\n",
    "env = 'data'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "ff = ff_utils.fdn_connection(key={\"default\" : tibanna.ff_keys})\n",
    "exclude_miseq = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 total number of sets\n",
      "1 sets completed\n",
      "\n",
      "22\n",
      "1 4DNESWWRKZDK\n",
      "DNaseI mouse\n",
      "4DNEXC8TTU1U part1 complete\n",
      "4DNEXC8TTU1U part2 complete\n",
      "4DNESWWRKZDK part3 complete\n",
      "\n",
      "5\n",
      "2 4DNESAGFJ56V skipping small file size 5\n",
      "\n",
      "79\n",
      "3 4DNES49IRGRY\n",
      "DNaseI mouse\n",
      "4DNEXYLI7G52 part1 complete\n",
      "4DNEXYLI7G52 part2 complete\n",
      "4DNES49IRGRY part3 complete\n",
      "\n",
      "92\n",
      "4 4DNESNPYVXDM\n",
      "DNaseI mouse\n",
      "4DNEXBH5B94I part1 complete\n",
      "4DNEXBH5B94I part2 complete\n",
      "4DNESNPYVXDM part3 complete\n",
      "\n",
      "59\n",
      "5 4DNESF8PGGTI\n",
      "DNaseI mouse\n",
      "4DNEX8XQA6B5 part1 complete\n",
      "4DNEX8XQA6B5 part2 complete\n",
      "4DNESF8PGGTI part3 complete\n",
      "\n",
      "84\n",
      "6 4DNESG15UT75\n",
      "DNaseI mouse\n",
      "4DNEX3IVMKFZ part1 complete\n",
      "4DNEX3IVMKFZ part2 complete\n",
      "4DNEXXX48QKL part1 complete\n",
      "4DNEXXX48QKL part2 complete\n",
      "4DNEXSURRHM7 part1 complete\n",
      "4DNEXSURRHM7 part2 complete\n",
      "4DNESG15UT75 part3 complete\n",
      "\n",
      "42\n",
      "7 4DNESZFHB53P\n",
      "DNaseI human\n",
      "4DNEX778XE4J part1 complete\n",
      "4DNEX778XE4J part2 complete\n",
      "4DNEXEKTHDKQ part1 complete\n",
      "4DNEXEKTHDKQ part2 complete\n",
      "4DNESZFHB53P part3 complete\n",
      "\n",
      "39\n",
      "8 4DNESGTHHJAC\n",
      "DNaseI human\n",
      "4DNEXGBN93KT part1 complete\n",
      "4DNEXGBN93KT part2 complete\n",
      "4DNEXIIJKSEL part1 complete\n",
      "4DNEXIIJKSEL part2 complete\n",
      "4DNESGTHHJAC part3 complete\n",
      "\n",
      "33\n",
      "9 4DNES6G787KF\n",
      "DNaseI human\n",
      "4DNEXRGPHY4P part1 complete\n",
      "4DNEXRGPHY4P part2 complete\n",
      "4DNEXGZH6KEK part1 complete\n",
      "4DNEXGZH6KEK part2 complete\n",
      "4DNES6G787KF part3 complete\n",
      "\n",
      "30\n",
      "10 4DNESSU7RRNR\n",
      "DNaseI human\n",
      "4DNEXR5M1VEX part1 complete\n",
      "4DNEXR5M1VEX part2 complete\n",
      "4DNEXWZTQFYP part1 complete\n",
      "4DNEXWZTQFYP part2 complete\n",
      "4DNESSU7RRNR part3 complete\n",
      "\n",
      "26\n",
      "11 4DNESW72J62T\n",
      "DNaseI human\n",
      "4DNEXWPS7HX4 part1 complete\n",
      "4DNEXWPS7HX4 part2 complete\n",
      "4DNEX2BP2PR5 part1 complete\n",
      "4DNEX2BP2PR5 part2 complete\n",
      "4DNESW72J62T is missing Part3\n",
      "\n",
      "191\n",
      "12 4DNESRCFT5AI\n",
      "MNase human\n",
      "4DNEXQMEU2O4 part1 complete\n",
      "4DNEXQMEU2O4 part2 complete\n",
      "4DNEXWHALJ1K part1 complete\n",
      "4DNEXWHALJ1K part2 complete\n",
      "4DNEX9JAVMKE part1 complete\n",
      "4DNEX9JAVMKE part2 complete\n",
      "4DNEX24VTARO part1 complete\n",
      "4DNEX24VTARO part2 complete\n",
      "4DNESRCFT5AI part3 complete\n",
      "\n",
      "275\n",
      "13 4DNESCZJD9KK\n",
      "MNase human\n",
      "4DNEXC1TYVLD part1 complete\n",
      "4DNEXC1TYVLD part2 complete\n",
      "4DNEX9TF73VI part1 complete\n",
      "4DNEX9TF73VI part2 complete\n",
      "4DNEXRJYVOZJ part1 complete\n",
      "4DNEXRJYVOZJ part2 complete\n",
      "4DNEXONC5BJ1 part1 complete\n",
      "4DNEXONC5BJ1 part2 complete\n",
      "4DNEX1MDLILR part1 complete\n",
      "4DNEX1MDLILR part2 complete\n",
      "4DNEX5F27GL2 part1 complete\n",
      "4DNEX5F27GL2 part2 complete\n",
      "4DNESCZJD9KK part3 complete\n",
      "11\n",
      "[u'4DNESWWRKZDK', u'4DNES49IRGRY', u'4DNESNPYVXDM', u'4DNESF8PGGTI', u'4DNESG15UT75', u'4DNESZFHB53P', u'4DNESGTHHJAC', u'4DNES6G787KF', u'4DNESSU7RRNR', u'4DNESRCFT5AI', u'4DNESCZJD9KK']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# for a given experiment set and some parameters like instrument\n",
    "# print set of files and their partA hic workflow status\n",
    "# if there are one that are running report the number of running cases\n",
    "# if there are file pairs that don't have a corresponding part A, report them separately\n",
    "\n",
    "out_n = \"This is an output file of the Hi-C processing pipeline\"\n",
    "int_n = \"This is an intermediate file in the HiC processing pipeline\"\n",
    "\n",
    "def step_settings(seq, my_organism):\n",
    "    genome = \"\"\n",
    "    mapper = {'human':'GRCh38','mouse':'GRCm38'}\n",
    "    genome = mapper.get(my_organism)\n",
    "    \n",
    "    wf_dict =[{\n",
    "        'wf_name': 'bwa-mem',\n",
    "        'wf_uuid': '3feedadc-50f9-4bb4-919b-09a8b731d0cc',\n",
    "        'parameters': {\"nThreads\": 16},\n",
    "        'custom_pf_fields': {\n",
    "            'out_bam': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'intermediate file',\n",
    "                'description': int_n}\n",
    "        }},\n",
    "        {\n",
    "        'wf_name': 'hi-c-processing-bam',\n",
    "        'wf_uuid': '023bfb3e-9a8b-42b9-a9d4-216079526f68',\n",
    "        'parameters': {\"nthreads_merge\": 16, \"nthreads_parse_sort\": 16},\n",
    "        'custom_pf_fields': {\n",
    "            'annotated_bam': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'alignment',\n",
    "                'description': out_n},\n",
    "            'filtered_pairs': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact list-replicate',\n",
    "                'description': out_n}\n",
    "        }},\n",
    "        {\n",
    "        'wf_name': 'hi-c-processing-pairs-nore',\n",
    "        'wf_uuid': 'c19ee11e-9d5a-454f-af50-600a0cf990b6',\n",
    "        'parameters': {\"nthreads\": 1, \"maxmem\": \"32g\"},\n",
    "        'custom_pf_fields': {\n",
    "            'cooler_normvector': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'juicebox norm vector',\n",
    "                'description': out_n},\n",
    "            'hic': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact matrix',\n",
    "                'description': out_n},\n",
    "            'mcool': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact matrix',\n",
    "                'description': out_n},\n",
    "            'merged_pairs': {\n",
    "                'genome_assembly': genome,\n",
    "                'file_type': 'contact list-combined',\n",
    "                'description': out_n}\n",
    "        }}]\n",
    "    \n",
    "    return wf_dict[seq]\n",
    "\n",
    "\n",
    "# url for micro-C Dnase exps\n",
    "exp_types = ['micro-C', 'DNase%20Hi-C']\n",
    "set_url = '/search/?'+ \\\n",
    "            '&'.join(['experiments_in_set.experiment_type='+i for i in exp_types])+ \\\n",
    "            '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "            '&status=released&status=released%20to%20project'\n",
    "\n",
    "run_sets = ff_utils.search_metadata(set_url , ff_env=env)\n",
    "\n",
    "add_pc = True\n",
    "add_tag = True\n",
    "\n",
    "add_rel = False\n",
    "add_wfr = False\n",
    "\n",
    "\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "all_sets = len(run_sets)\n",
    "print(str(all_sets)+' total number of sets')\n",
    "\n",
    "run_sets = [i for i in run_sets if \"HiC_Pipeline_0.2.5\"  not in i.get('completed_processes', [])]\n",
    "print(str(all_sets-len(run_sets))+ ' sets completed')\n",
    "\n",
    "for a_set in run_sets: \n",
    "    counter += 1\n",
    "\n",
    "    print \n",
    "    fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size, lab = find_pairs(a_set, exclude_miseq, env, tibanna)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print f_size\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not bwa_ref or not chrsize_ref:\n",
    "        print counter, a_set['accession'], organism, enzyme, 'skipping set with not chrsize/bwa index'\n",
    "        continue\n",
    "    \n",
    "    if f_size < 15:\n",
    "        print counter, a_set['accession'], 'skipping small file size', str(f_size) \n",
    "        continue\n",
    "        \n",
    "        \n",
    "    print counter, a_set['accession']\n",
    "    print enzyme, organism\n",
    "    part3 = 'done'\n",
    "    list_release = []\n",
    "    set_pairs = []        \n",
    "    # cycle through the experiments\n",
    "    for exp in fastqpairs.keys():\n",
    "        if not fastqpairs.get(exp):\n",
    "            print(exp, 'does not have any fastq pairs')\n",
    "            continue\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        part2 = 'done'\n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            bam1 = get_wfr_out(pair[0], 'bwa-mem 0.2.5', 'bam', env)\n",
    "            bam2 = get_wfr_out(pair[1], 'bwa-mem 0.2.5', 'bam', env)\n",
    "            # if run is not successful\n",
    "            if bam1.startswith('no') or not bam1 or bam1 != bam2:\n",
    "                part1 = 'not ready'\n",
    "                if add_wfr:\n",
    "                    if not bwa_index:\n",
    "                        print 'not yet usable', organism\n",
    "                        continue\n",
    "                    inp_f = {'fastq1':pair[0], 'fastq2':pair[1], 'bwa_index':bwa_ref}\n",
    "                    name_tag = pair[0].split('/')[2]+'_'+pair[1].split('/')[2]\n",
    "                    run_missing_wfr(step_settings(0, organism), inp_f, name_tag, env, tibanna)\n",
    "            elif bam1 == 'running':\n",
    "                part1 = 'still running'\n",
    "                print('part1 still running')\n",
    "            # if successful\n",
    "            else:\n",
    "                exp_bams.append(bam1)\n",
    "                list_release.append(bam1)\n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            print exp, 'has missing Part1 runs'\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print exp, 'part1 complete'\n",
    "        #check if part 2 is run already, it not start the run\n",
    "        exp_com_bam = []\n",
    "        exp_pairs = []\n",
    "        for bam in exp_bams:\n",
    "            com_bam = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', 'bam', env)\n",
    "            pairs = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', 'pairs', env)\n",
    "            # try to run if missing\n",
    "            if pairs.startswith('no') or not pairs:\n",
    "                part2 = 'not ready'\n",
    "                part3 = 'not ready'\n",
    "                \n",
    "            elif pairs == 'running':\n",
    "                part2 = 'still running'\n",
    "                part3 = 'not ready'\n",
    "                \n",
    "            else:\n",
    "                exp_com_bam.append(com_bam)\n",
    "                exp_pairs.append(pairs)\n",
    "                \n",
    "        # if still running, skip to next experiment\n",
    "        if part2 == 'still running':\n",
    "            print('part2 still running')\n",
    "            continue\n",
    "        \n",
    "        # make sure all bams went through the same wfr and produces same file\n",
    "        if part2 != 'done' or len(list(set(exp_com_bam))) != 1 or len(list(set(exp_pairs))) !=1:\n",
    "            print exp, 'Part2 did not complete'\n",
    "            part3 = 'not ready' \n",
    "        \n",
    "            if add_wfr:\n",
    "                if not chrsize_ref:\n",
    "                    print 'not yet usable', organism\n",
    "                    continue\n",
    "                # make sure no duplicates\n",
    "                inp_f = {'input_bams':exp_bams, 'chromsize':chrsize_ref}           \n",
    "                run_missing_wfr(step_settings(1, organism), inp_f, exp, env, tibanna)   \n",
    "            continue\n",
    "            \n",
    "        # add bam and pairs to exp proc file\n",
    "        list_release.extend([exp_com_bam[0],exp_pairs[0]])\n",
    "        if add_pc:\n",
    "            add_preliminary_processed_files(exp, [exp_com_bam[0],exp_pairs[0]], env)\n",
    "        \n",
    "        print exp, 'part2 complete'\n",
    "        set_pairs.append(exp_pairs[0])\n",
    "    \n",
    "    if part3 != 'done':\n",
    "        print 'Part3 not ready'\n",
    "        continue\n",
    "    \n",
    "    if not set_pairs:\n",
    "        print 'no pairs can be produced from this set'\n",
    "        continue\n",
    "        \n",
    "    merged_pairs = []\n",
    "    for set_pair in set_pairs:\n",
    "        merged_pair = get_wfr_out(set_pair, 'hi-c-processing-pairs-nore 0.2.5', 'pairs', env)\n",
    "        hic = get_wfr_out(set_pair, 'hi-c-processing-pairs-nore 0.2.5', 'hic', env)\n",
    "        mcool = get_wfr_out(set_pair, 'hi-c-processing-pairs-nore 0.2.5', 'mcool', env)\n",
    "        normvec = get_wfr_out(set_pair, 'hi-c-processing-pairs-nore 0.2.5', 'normvector_juicerformat', env)\n",
    "        if merged_pair.startswith('no') or not merged_pair:\n",
    "            part3 = 'not ready'\n",
    "            break\n",
    "        elif merged_pair == 'running':\n",
    "            part3 = 'still running'\n",
    "            break\n",
    "        else:\n",
    "            merged_pairs.append(merged_pair)\n",
    "    \n",
    "    \n",
    "    # if part3 is still running report it, and skip the rest of the script\n",
    "    if part3 == 'still running':\n",
    "        print 'part3', part3\n",
    "        continue        \n",
    "                \n",
    "    if part3 != 'done' or len(list(set(merged_pairs))) != 1:\n",
    "        print a_set['accession'], 'is missing Part3'\n",
    "        \n",
    "        # if it is not run, and add_wfr is true, go for it, then skip the rest of the script\n",
    "        if add_wfr:\n",
    "            if not chrsize_ref:\n",
    "                print 'not yet usable', organism\n",
    "                continue\n",
    "\n",
    "            inp_f = {'input_pairs':set_pairs, 'chromsizes':chrsize_ref} \n",
    "            run_missing_wfr(step_settings(2, organism), inp_f, a_set['accession'], env, tibanna)\n",
    "        continue\n",
    "    #####\n",
    "    #add competed flag to experiment\n",
    "    if add_tag:\n",
    "        ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.5\"]}, obj_id=a_set['accession'] , ff_env=env)\n",
    "    \n",
    "    # add processed files to set\n",
    "    list_release.extend([merged_pair, hic, mcool, normvec])\n",
    "    if add_pc:\n",
    "        add_preliminary_processed_files(a_set['accession'], [merged_pair, hic, mcool], env)\n",
    "    \n",
    "    #release files and wfrs\n",
    "    if add_rel:\n",
    "        release_files(a_set['accession'], list(set(list_release)), env)\n",
    "    \n",
    "    completed += 1\n",
    "    completed_acc.append(a_set['accession'])\n",
    "    print a_set['accession'], 'part3 complete'\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "print completed\n",
    "print completed_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'other_processed_files': [{'files': [u'/files-processed/4DNFIPCU2LGI/', u'/files-processed/4DNFI1KTTETW/'], 'type': 'preliminary', 'title': 'HiC Processing Pipeline - Preliminary Files'}]}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
