{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 files missing from s3\n",
      "0 files had patch problems\n",
      "0 uploading files skipped\n",
      "0 total files patched\n"
     ]
    }
   ],
   "source": [
    "from dcicutils import ff_utils\n",
    "from dcicutils import s3Utils\n",
    "# for a given experiment set and some parameters like instrument\n",
    "# print set of files and their partA hic workflow status\n",
    "# if there are one that are running report the number of running cases\n",
    "# if there are file pairs that don't have a corresponding part A, report them separately\n",
    "\n",
    "my_env = 'data'\n",
    "my_auth = ff_utils.get_authentication_with_server({}, ff_env = my_env)\n",
    "\n",
    "my_s3_util = s3Utils(env=my_env)\n",
    "raw_bucket = my_s3_util.raw_file_bucket\n",
    "out_bucket = my_s3_util.outfile_bucket\n",
    "\n",
    "file_not_found = 0\n",
    "uploading = 0\n",
    "success = 0\n",
    "patch_problem = 0\n",
    "\n",
    "file_url = '/search/?type=File&file_size=No+value&limit=all'+ \\\n",
    "           '&status=uploaded&status=released+to+project&status=released'\n",
    "files_resp = ff_utils.search_metadata(file_url, key = my_auth)\n",
    "print len(files_resp)\n",
    "\n",
    "no_size = 0\n",
    "counter = 0\n",
    "for a_file in files_resp:\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print counter\n",
    "    if a_file['status'] in ['uploading', 'to be uploaded by workflow']:\n",
    "        uploading += 1\n",
    "        continue\n",
    "    # check if there is  no filesize\n",
    "    if not a_file.get('file_size'):\n",
    "        # decide on the bucket\n",
    "        if 'FileProcessed' in a_file['@type']:\n",
    "            bucket = out_bucket\n",
    "        else:\n",
    "            bucket = raw_bucket\n",
    "            \n",
    "        # check if file is in s3\n",
    "        head_info = my_s3_util.does_key_exist(a_file['upload_key'], bucket)\n",
    "        if not head_info:\n",
    "            print a_file['@id'], a_file['status']\n",
    "            file_not_found += 1\n",
    "            continue\n",
    "        file_size = head_info['ContentLength']\n",
    "\n",
    "        patch_data = {'file_size': file_size}\n",
    "        try:\n",
    "            ff_utils.patch_metadata(patch_data, obj_id=a_file['uuid'] ,key=my_auth)\n",
    "            success += 1\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            print\n",
    "            patch_problem += 1\n",
    "\n",
    "print file_not_found, 'files missing from s3'\n",
    "print patch_problem, 'files had patch problems'\n",
    "print uploading, 'uploading files skipped'\n",
    "print success, 'total files patched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
